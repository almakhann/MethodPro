---
title: "Titanic "
output: html_document
---
##Almakhan Serik
####29.06.2018

## Competition Description

 The sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.

 One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.

 In this challenge, we ask you to complete the analysis of what sorts of people were likely to survive. In particular, we ask you to apply the tools of machine learning to predict which passengers survived the tragedy.
 
 
#### Set Direction
```{r, echo=TRUE, message=FALSE}
setwd("~/Desktop/Titanic")
```

#### Used Libraries
```{r, echo=TRUE, message=FALSE}
library(PerformanceAnalytics)
library(rpart)
library(randomForest)
library(ROSE)
library(car)
library(caret)
library(lubridate)
library(corrplot)
library(psych)
library(dplyr)
library(missForest)
library(Matrix)
library(MatrixModels)
library(xgboost)
library(PerformanceAnalytics)

```

#### Train Data 
 Data manipulation firstly changed all integer values to numeric.Then removed columns which are character and I decided that they are unused. na.omit takes data without missing values
```{r}
train <- read.csv("train.csv")
train <- train %>% mutate_if(is.integer, as.numeric)

df <- train[,-c(1,4,9,11,12)]
df <- na.omit(df)
str(df)
```

#### Test Data
MISSFOREST-is used to impute missing values particularly in the case of mixed-type data. 
Formula: (mean((Xtrue - Ximp)/var(Xtrue))^0.5
```{r,echo=TRUE,results='hide'}
testData <- read.csv("test.csv")
testData <- testData %>% mutate_if(is.integer, as.numeric)

df_test <- testData[,-c(1,3,8,10,11)]

d <- missForest(df_test)
df_test <- d$ximp
d$OOBerror
sum(is.na(df_test))

```

Divide data into two part(80% - 20%), first part to make search, second to test data which predicted 
```{r}
index <- createDataPartition(df$Survived,p=0.8, list = F)
train <- df[index,]
test <- df[-index,]

```

#### Corelation datas
```{r}
a <- train
a$Sex <- as.numeric(a$Sex)
chart.Correlation(a)
corr.test(a)
plot(a)
```

### Used functions
```{r}
# Function which is used to save data 
saveData <- function(x){
  df_test$Survived <- predict(x,df_test)
  df_test$Survived <- ifelse(df_test$Survived >= 0.5,1,0)

  new_data <- data.frame(testData[,"PassengerId"])
  new_data$Survived <- df_test[,"Survived"]
  write.csv(new_data, "myresult.csv", row.names=FALSE)
}

# Gives RMSE test checker result also returns ADJUST R SQUARE result
checkData <- function(x){
  pred <- predict(x,test)
  print (RMSE(test$Survived,pred))
  
  pred <- predict(x,test)
  pred <- ifelse(pred >= 0.5,1,0)
  print(RMSE(test$Survived,pred))
  paste0("Adj Rsquare: ", (summary(x))$adj.r.squared)
}

checkDt <- function(x){
  pred <- predict(x,test)
  print (RMSE(test$Survived,pred))
  
  pred <- predict(x,test)
  pred <- ifelse(pred >= 0.5,1,0)
  print(RMSE(test$Survived,pred))
}
```

### Linear Regression
 linear regression is a linear approach to modelling the relationship between a  dependent variable and one or more independent variables. 
```{r}
lm <- lm(Survived~.,train)
checkData(lm)
```

Tried many linear regression models also used step function to get ideal model. By running a differnt type of models i learn which value is import,also tried to take the secondary relationship to get higher value og Adjust R square value
```{r,echo=TRUE, message=FALSE,results='hide'}
# To find ideal value by STEP() function
ideal <- step(lm,direction = "backward")

lm1 <- lm(Survived~(Pclass+Age+Sex*Age+SibSp*Fare)^2,train)
checkData(lm1)

lm0 <- lm(Survived ~ (Pclass + Sex + Age + SibSp)^2,train)
checkData(lm0)

lm2 <- lm(Survived~(Pclass+SibSp*Parch+Age*Fare+Sex*Fare)^2,train)
checkData(lm2)

lm3 <- glm(Survived~(Pclass+Age+Sex*Age+SibSp)^3,train, family = "binomial")
checkData(lm3)

# Result -> 0.78468
lm4 <- lm(Survived~(Pclass*Sex+Sex*Age+Age*Fare+SibSp*Parch)^2,train)
checkData(lm4)

lm5 <- lm(Survived~(Pclass*Sex+Sex*Age+SibSp*Parch)^2,train)
checkData(lm5)
```
In sixth try i got my highest result on competitions
```{r}
# My Top result -> 0.78947
lm6 <- lm(Survived~(Pclass*Sex+Sex*Age+SibSp*Parch)^3,train)
checkData(lm6)

lm7 <- lm(Survived~(Pclass+Sex+Parch+Age+Parch*SibSp)^3,train)
checkData(lm7)
```

```{r,echo=TRUE, message=FALSE,results='hide'}
step(lm7,direction = "backward")

# Result -> 0.78468
lm8 <- lm(formula = Survived ~ (Pclass + Sex + Age + SibSp + Parch)^3,train)
checkData(lm8)
```

#### Desicion Tree
A decision tree is a decision support tool that uses a tree-like graph or model of decisions and their possible consequences, including chance event outcomes, resource costs, and utility. It is one way to display an algorithm that only contains conditional control statements.
```{r}
dt <- rpart(Survived~.,train)
checkDt(dt)
# Result -> 0.73684
dt1 <- rpart(Survived ~ (Pclass + Sex + Age + SibSp + Parch), train)
checkDt(dt1)
```

#### Random Forest
```{r}
# Result = 0.77990
rf <- randomForest(Survived~(Pclass+Sex*Age+SibSp+Age*Parch+Pclass*Fare)^2,train,
                       type="regression",
                       ntree=10,
                       do.trace=F)
checkDt(rf)

# Result -> 0.75598
rf1 <- randomForest(Survived~.,train,
                   type="classification",
                   ntree=1000,
                   do.trace=F)
checkDt(rf1)


rf2 <- randomForest(Survived~(Pclass+Sex+Age+SibSp*Parch),train,
                   type="regression",
                   ntree=1000,
                   do.trace=F)
checkDt(rf2)
```


#### XGBoost
```{r}
train_matrix <- data.matrix(select(train,-Survived))
test_matrix <- data.matrix(select(test,-Survived))

train_target <- train$Survived
test_target <- test$Survived

dtrain <- xgb.DMatrix(data=train_matrix,label=train_target)
ctest <- xgb.DMatrix(data=test_matrix,label=test_target)

watchlist <- list(train=dtrain,test=ctest)
xgb <- xgb.train(data=dtrain,
                 nround=50,
                 maximize = FALSE,
                 early_stopping_rounds = 5,
                 watchlist = watchlist,
                 max_depth=6,
                 #objective = "reg:linear",
                 eval_metric = "rmse",
                 alpha=0.01,
                 lambda=0.01,
                 colsample_bytree=0.7,
                 subsample = 0.7
)
pred_xgb <- predict(xgb,ctest)
RMSE(test_target,pred_xgb)

```


